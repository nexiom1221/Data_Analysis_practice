{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-68-937395115e45>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'lxml'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mcontents_table\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"table\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mtable_body\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontents_table\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"tbody\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mtable_rows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtable_body\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"tr\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m# a태그의 href 속성을 리스트로 추출하여, 크롤링 할 페이지 리스트를 생성합니다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# 크롤링할 사이트 주소를 정의합니다.\n",
    "source_url = \"https://namu.wiki/RecentChanges\"\n",
    "\n",
    "# 사이트의 html 구조에 기반하여 크롤링을 수행합니다.\n",
    "req = requests.get(source_url)\n",
    "html = req.content\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "contents_table = soup.find(name=\"table\")\n",
    "table_body = contents_table.find(name=\"tbody\")\n",
    "table_rows = table_body.find_all(name=\"tr\")\n",
    "# a태그의 href 속성을 리스트로 추출하여, 크롤링 할 페이지 리스트를 생성합니다.\n",
    "page_url_base = \"https://namu.wiki\"\n",
    "page_urls = []\n",
    "for index in range(0, len(table_rows)):\n",
    "    first_td = table_rows[index].find_all('td')[0]\n",
    "    td_url = first_td.find_all('a')\n",
    "    if len(td_url) > 0:\n",
    "        page_url = page_url_base + td_url[0].get('href')\n",
    "        if 'png' not in page_url:\n",
    "            page_urls.append(page_url)\n",
    "\n",
    "# 중복 url을 제거합니다.\n",
    "page_urls = list(set(page_urls))\n",
    "for page in page_urls[:5]:\n",
    "    print(page)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-69-a8f58f49d310>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'lxml'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mcontents_table\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"article\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mtitle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontents_table\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'h1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0mcategory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontents_table\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ul'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mcontent_paragraphs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontents_table\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"div\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"class\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m\"wiki-paragraph\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "# 크롤링한 데이터를 데이터 프레임으로 만들기 위해 준비합니다.\n",
    "columns = ['title', 'category', 'content_text']\n",
    "df = pd.DataFrame(columns=columns)\n",
    "\n",
    "# 각 페이지별 '제목', '카테고리', '본문' 정보를 데이터 프레임으로 만듭니다.\n",
    "for page_url in page_urls:\n",
    "\n",
    "    # 사이트의 html 구조에 기반하여 크롤링을 수행합니다.\n",
    "    req = requests.get(page_url)\n",
    "    html = req.content\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    contents_table = soup.find(name=\"article\")\n",
    "    title = contents_table.find_all('h1')[0]\n",
    "    category = contents_table.find_all('ul')[0]\n",
    "    content_paragraphs = contents_table.find_all(name=\"div\", attrs={\"class\":\"wiki-paragraph\"})\n",
    "    content_corpus_list = []\n",
    "    \n",
    "for paragraphs in content_paragraphs:\n",
    "    content_corpus_list.append(paragraphs.text)\n",
    "content_corpus = \"\".join(content_corpus_list)\n",
    "\n",
    "print(title.text)\n",
    "print(\"\\n\")\n",
    "print(category.text)\n",
    "print(\"\\n\")\n",
    "print(content_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-70-20acd0666b3a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mcontents_table\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"article\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m#contents_table = soup.find(name=\"table\", attrs={\"class\":\"table-hover\"})\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mtable_body\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontents_table\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"tbody\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[0mtable_rows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtable_body\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"tr\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "source_url = \"https://namu.wiki/RecentChanges\"\n",
    "\n",
    "req = requests.get(source_url)\n",
    "html = req.content\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "contents_table = soup.find(name=\"article\")\n",
    "#contents_table = soup.find(name=\"table\", attrs={\"class\":\"table-hover\"})\n",
    "table_body = contents_table.find(name=\"tbody\")\n",
    "table_rows = table_body.find_all(name=\"tr\")\n",
    "\n",
    "# a 태그의 href 속성을 리스트로 추출, 크롤링 페이지 리스트 생성\n",
    "page_url_base = \"https://namu.wiki\"\n",
    "page_urls = []\n",
    "for index in range(0, len(table_rows)):\n",
    "    first_td = table_rows[index].find_all('td')[0]\n",
    "    td_url = first_td.find_all('a')\n",
    "    if len(td_url) >0 :\n",
    "        page_url = page_url_base + td_url[0].get('href')\n",
    "        page_urls.append(page_url)\n",
    "\n",
    "# 중복 url 제거\n",
    "page_urls = list(set(page_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-71-84cf8535f1b2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'lxml'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mcontents_table\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"table\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"class\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m\"table-hover\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mtitle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontents_table\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'h1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "# 크롤링한 데이터를 데이터 프레임으로 만들기 위해 준비합니다.\n",
    "columns = ['title', 'category', 'content_text']\n",
    "df = pd.DataFrame(columns=columns)\n",
    "\n",
    "# 각 페이지별 '제목', '카테고리', '본문' 정보를 데이터 프레임으로 만듭니다.\n",
    "for page_url in page_urls:\n",
    "\n",
    "    # 사이트의 html 구조에 기반하여 크롤링을 수행합니다.\n",
    "    req = requests.get(page_url)\n",
    "    html = req.content\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    contents_table = soup.find(name=\"table\", attrs={\"class\":\"table-hover\"})\n",
    "    title = contents_table.find_all('h1')[0]\n",
    "   \n",
    "\n",
    "    # 카테고리 정보가 없는 경우를 확인합니다.\n",
    "    if len(contents_table.find_all('ul')) > 0:\n",
    "        category = contents_table.find_all('ul')[0]\n",
    "    else:\n",
    "        category = None\n",
    "        \n",
    "    content_paragraphs = contents_table.find_all(name=\"div\", attrs={\"class\":\"wiki-paragraph\"})\n",
    "    content_corpus_list = []\n",
    "    \n",
    "    # 페이지 내 제목 정보에서 개행 문자를 제거한 뒤 추출합니다. 만약 없는 경우, 빈 문자열로 대체합니다.\n",
    "    if title is not None:\n",
    "        row_title = title.text.replace(\"\\n\", \" \")\n",
    "    else:\n",
    "        row_title = \"\"\n",
    "    \n",
    "    # 페이지 내 본문 정보에서 개행 문자를 제거한 뒤 추출합니다. 만약 없는 경우, 빈 문자열로 대체합니다.\n",
    "    if content_paragraphs is not None:\n",
    "        for paragraphs in content_paragraphs:\n",
    "            if paragraphs is not None:\n",
    "                content_corpus_list.append(paragraphs.text.replace(\"\\n\", \" \"))\n",
    "            else:\n",
    "                content_corpus_list.append(\"\")\n",
    "    else:\n",
    "        content_corpus_list.append(\"\")\n",
    "        \n",
    "    # 페이지 내 카테고리정보에서 “분류”라는 단어와 개행 문자를 제거한 뒤 추출합니다. 만약 없는 경우, 빈 문자열로 대체합니다.\n",
    "    if category is not None:\n",
    "        row_category = category.text.replace(\"\\n\", \" \")\n",
    "    else:\n",
    "        row_category = \"\"\n",
    "    \n",
    "    # 모든 정보를 하나의 데이터 프레임에 저장합니다.\n",
    "    row = [row_title, row_category, \"\".join(content_corpus_list)]\n",
    "    series = pd.Series(row, index=df.columns)\n",
    "    df = df.append(series, ignore_index=True)\n",
    "    \n",
    "    df.head(5)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html>\n",
      "<head>\n",
      "<meta charset=\"utf-8\">\n",
      "<meta name=\"viewport\" content=\"width=1240\">\n",
      "<title>비정상적인 트래픽 감지</title>\n",
      "<style>\n",
      "section {\n",
      "\tposition: fixed;\n",
      "\ttop: 0;\n",
      "\tright: 0;\n",
      "\tbottom: 0;\n",
      "\tleft: 0;\n",
      "\tpadding: 80px 0 0;\n",
      "\tbackground-color:#EFEFEF;\n",
      "\tfont-family: \"Open Sans\", sans-serif;\n",
      "\ttext-align: center;\n",
      "}\n",
      "h1 {\n",
      "\tmargin: 0 0 19px;\n",
      "\tfont-size: 40px;\n",
      "\tfont-weight: normal;\n",
      "\tcolor: #E02B2B;\n",
      "\tline-height: 40px;\n",
      "}\n",
      "p {\n",
      "\tmargin: 0 0 0 0;\n",
      "\tfont-size: 16px;\n",
      "\tcolor:#444;\n",
      "\tline-height: 23px;\n",
      "}\n",
      "form {\n",
      "\tdisplay: inline-block;\n",
      "}\n",
      "</style>\n",
      "<script src=\"//www.google.com/recaptcha/api.js\" async defer></script>\n",
      "</head>\n",
      "<body>\n",
      "<section>\n",
      "<h1>비정상적인 트래픽 감지</h1>\n",
      "<p>\n",
      "시스템이 컴퓨터 네트워크에서 비정상적인 트래픽을 감지했습니다.<br>이 페이지는 로봇이 아니라 실제 사용자가 요청을 보내고 있는지를 확인하는 페이지입니다.\n",
      "</p>\n",
      "<p>\n",
      "이 화면이 지속적으로 표시될 경우 브라우져의 모든 탭(공유기 환경일 경우 접속된 모든 기기의 탭)을 종료 후 다시 접속하시기 바랍니다.\n",
      "</p>\n",
      "<form action=\"/cgi-bin/check\" method=\"POST\">\n",
      "<input type=\"hidden\" name=\"redirect\" value=\"/RecentChanges\">\n",
      "<div class=\"g-recaptcha\" data-sitekey=\"6LcUt7kUAAAAAI-n3Y-xhmhy0IvAJE26ltQes6md\"></div>\n",
      "<input type=\"submit\" value=\"확인\">\n",
      "</form>\n",
      "</section>\n",
      "</body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "import requests \n",
    "url = 'https://namu.wiki/RecentChanges'\n",
    "req = requests.get(url)\n",
    "print(req.content.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
